{"paragraphs":[{"text":"%sh\n\nmkdir /tmp/tweets\nrm -rf /tmp/tweets/*\ncd /tmp/tweets\nwget -O /tmp/tweets/tweets.zip https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/hdp-2.6/sentiment-analysis-with-apache-spark/assets/tweets.zip\nunzip /tmp/tweets/tweets.zip\nrm /tmp/tweets/tweets.zip\n\n# Remove existing (if any) copy of data from HDFS. You could do this with Ambari file view.\nhdfs dfs -mkdir /tmp/tweets_staging/\nhdfs dfs -rmr -f /tmp/tweets_staging/* -skipTrash\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/tweets/* /tmp/tweets_staging\n\n","user":"anonymous","dateUpdated":"2017-05-25T03:08:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mkdir: cannot create directory `/tmp/tweets': File exists\n--2017-05-25 03:08:48--  https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/hdp-2.6/sentiment-analysis-with-apache-spark/assets/tweets.zip\nResolving raw.githubusercontent.com... 151.101.20.133\nConnecting to raw.githubusercontent.com|151.101.20.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1126458 (1.1M) [application/zip]\nSaving to: “/tmp/tweets/tweets.zip”\n\n     0K .......... .......... .......... .......... ..........  4%  579K 2s\n    50K .......... .......... .......... .......... ..........  9%  930K 1s\n   100K .......... .......... .......... .......... .......... 13%  768K 1s\n   150K .......... .......... .......... .......... .......... 18%  794K 1s\n   200K .......... .......... .......... .......... .......... 22% 4.71M 1s\n   250K .......... .......... .......... .......... .......... 27% 2.74M 1s\n   300K .......... .......... .......... .......... .......... 31% 1.55M 1s\n   350K .......... .......... .......... .......... .......... 36%  977K 1s\n   400K .......... .......... .......... .......... .......... 40%  562K 1s\n   450K .......... .......... .......... .......... .......... 45%  657K 1s\n   500K .......... .......... .......... .......... .......... 49%  832K 1s\n   550K .......... .......... .......... .......... .......... 54% 1002K 1s\n   600K .......... .......... .......... .......... .......... 59% 1.25M 0s\n   650K .......... .......... .......... .......... .......... 63%  647K 0s\n   700K .......... .......... .......... .......... .......... 68% 1.20M 0s\n   750K .......... .......... .......... .......... .......... 72% 2.97M 0s\n   800K .......... .......... .......... .......... .......... 77% 1.09M 0s\n   850K .......... .......... .......... .......... .......... 81% 1.47M 0s\n   900K .......... .......... .......... .......... .......... 86% 1.03M 0s\n   950K .......... .......... .......... .......... .......... 90%  931K 0s\n  1000K .......... .......... .......... .......... .......... 95% 1.40M 0s\n  1050K .......... .......... .......... .......... .......... 99% 2.91M 0s\n  1100K                                                       100%  108G=1.1s\n\n2017-05-25 03:08:49 (1.01 MB/s) - “/tmp/tweets/tweets.zip” saved [1126458/1126458]\n\nArchive:  /tmp/tweets/tweets.zip\n  inflating: tweets-003256752.json   \n  inflating: tweets-003307775.json   \n  inflating: tweets-003319811.json   \n  inflating: tweets-003331841.json   \n  inflating: tweets-003343855.json   \n  inflating: tweets-003355878.json   \n  inflating: tweets-003407908.json   \n  inflating: tweets-003419924.json   \n  inflating: tweets-003431951.json   \n  inflating: tweets-003443966.json   \n  inflating: tweets-003455990.json   \n  inflating: tweets-003507013.json   \n  inflating: tweets-003518036.json   \n  inflating: tweets-003530062.json   \n  inflating: tweets-003542103.json   \n  inflating: tweets-003554122.json   \n  inflating: tweets-003606150.json   \n  inflating: tweets-003618172.json   \n  inflating: tweets-003629195.json   \n  inflating: tweets-003642220.json   \n  inflating: tweets-003655247.json   \n  inflating: tweets-003705260.json   \n  inflating: tweets-003715291.json   \n  inflating: tweets-003727314.json   \n  inflating: tweets-003740367.json   \n  inflating: tweets-003752398.json   \n  inflating: tweets-003803427.json   \n  inflating: tweets-003812440.json   \n  inflating: tweets-003823470.json   \n  inflating: tweets-003835482.json   \n  inflating: tweets-003848518.json   \n  inflating: tweets-003900553.json   \n  inflating: tweets-003911578.json   \n  inflating: tweets-233005799.json   \n  inflating: tweets-233018824.json   \n  inflating: tweets-233030847.json   \n  inflating: tweets-233040883.json   \n  inflating: tweets-233052926.json   \n  inflating: tweets-233103950.json   \n  inflating: tweets-233114966.json   \n  inflating: tweets-233127023.json   \n  inflating: tweets-233139072.json   \n  inflating: tweets-233149095.json   \n  inflating: tweets-233201118.json   \n  inflating: tweets-233213392.json   \n  inflating: tweets-233226637.json   \nmkdir: `/tmp/tweets_staging': File exists\nrmr: DEPRECATED: Please use 'rm -r' instead.\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003256752.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003256752.json1495681742078\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003307775.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003307775.json1495681742161\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003319811.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003319811.json1495681742248\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003331841.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003331841.json1495681742315\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003343855.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003343855.json1495681742374\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003355878.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003355878.json1495681742422\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003407908.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003407908.json1495681742508\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003419924.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003419924.json1495681742573\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003431951.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003431951.json1495681742628\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003443966.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003443966.json1495681742669\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003455990.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003455990.json1495681742703\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003507013.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003507013.json1495681742738\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003518036.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003518036.json1495681742784\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003530062.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003530062.json1495681742817\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003542103.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003542103.json1495681742883\n17/05/25 03:09:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003554122.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003554122.json1495681742942\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003606150.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003606150.json1495681742986\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003618172.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003618172.json1495681743030\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003629195.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003629195.json1495681743060\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003642220.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003642220.json1495681743107\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003655247.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003655247.json1495681743184\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003705260.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003705260.json1495681743263\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003715291.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003715291.json1495681743315\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003727314.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003727314.json1495681743364\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003740367.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003740367.json1495681743417\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003752398.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003752398.json1495681743451\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003803427.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003803427.json1495681743506\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003812440.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003812440.json1495681743545\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003823470.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003823470.json1495681743595\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003835482.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003835482.json1495681743645\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003848518.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003848518.json1495681743666\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003900553.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003900553.json1495681743711\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-003911578.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-003911578.json1495681743744\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233005799.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233005799.json1495681743779\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233018824.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233018824.json1495681743816\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233030847.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233030847.json1495681743853\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233040883.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233040883.json1495681743897\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233052926.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233052926.json1495681743931\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233103950.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233103950.json1495681743955\n17/05/25 03:09:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233114966.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233114966.json1495681743993\n17/05/25 03:09:04 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233127023.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233127023.json1495681744017\n17/05/25 03:09:04 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233139072.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233139072.json1495681744052\n17/05/25 03:09:04 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233149095.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233149095.json1495681744093\n17/05/25 03:09:04 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233201118.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233201118.json1495681744124\n17/05/25 03:09:04 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233213392.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233213392.json1495681744153\n17/05/25 03:09:04 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox.hortonworks.com:8020/tmp/tweets_staging/tweets-233226637.json' to trash at: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/tweets_staging/tweets-233226637.json1495681744181\n"}]},"apps":[],"jobName":"paragraph_1495680906436_591346781","id":"20170315-205558_857231102","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:08:48+0000","dateFinished":"2017-05-25T03:09:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:326"},{"text":"import org.apache.spark._\nimport org.apache.spark.rdd._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.mllib.feature.HashingTF\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport org.apache.spark._\nimport org.apache.spark.rdd._\nimport org.apache.spark.SparkContext._\nimport scala.util.{Success, Try}\n\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n    var tweetDF = sqlContext.read.json(\"hdfs:///tmp/tweets_staging/*\")\n   // tweetDF.printSchema()\n    tweetDF.show()\n","user":"anonymous","dateUpdated":"2017-05-25T03:13:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark._\n\nimport org.apache.spark.rdd._\n\nimport org.apache.spark.SparkContext._\n\nimport org.apache.spark.mllib.feature.HashingTF\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\n\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\n\nimport org.apache.spark._\n\nimport org.apache.spark.rdd._\n\nimport org.apache.spark.SparkContext._\n\nimport scala.util.{Success, Try}\n\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@355b5300\n\ntweetDF: org.apache.spark.sql.DataFrame = [_corrupt_record: string, created_time: string, created_unixtime: bigint, displayname: string, lang: string, msg: string, time_zone: string, tweet_id: bigint]\n+---------------+--------------------+----------------+---------------+----+--------------------+--------------------+------------------+\n|_corrupt_record|        created_time|created_unixtime|    displayname|lang|                 msg|           time_zone|          tweet_id|\n+---------------+--------------------+----------------+---------------+----+--------------------+--------------------+------------------+\n|           null|Wed Mar 15 00:32:...|   1489537963226|  meantforpeace|  en|ogmorgaan xo_rare...|Eastern Time (US ...|841809347768377344|\n|           null|Wed Mar 15 00:32:...|   1489537963229|        mushtxq|  en|Happy early birth...|              London|841809347780972544|\n|           null|Wed Mar 15 00:32:...|   1489537962590| Carla_Pereira2|  en|After 10 hrs on a...|                    |841809345100697600|\n|           null|Wed Mar 15 00:32:...|   1489537963311|     DillMaddie|  en|i just want to be...|                    |841809348124917761|\n|           null|Wed Mar 15 00:32:...|   1489537963397|oabeh7hdYeD5K8a|  ja|RT tsukiuta1 ☆Hap...|                    |841809348485558272|\n|           null|Wed Mar 15 00:32:...|   1489537962772|   Hicky_Randel|  en|FlemingYoung happ...|                    |841809345864192000|\n|           null|Wed Mar 15 00:32:...|   1489537963397|        yastan9|  en|RT RealTillIMetYo...|                    |841809348485496832|\n|           null|Wed Mar 15 00:32:...|   1489537963345|     s_willorsa| und|nikeplus nikeplus...|                    |841809348267409408|\n|           null|Wed Mar 15 00:32:...|   1489537963371| queennatalia__|  en|RT cespedesj_ So ...|Eastern Time (US ...|841809348376567809|\n|           null|Wed Mar 15 00:32:...|   1489537963382|     YoungFloat|  en|RT SonnyMCFly_ Yo...|Atlantic Time (Ca...|841809348422729728|\n|           null|Wed Mar 15 00:32:...|   1489537963431|      rissamo06|  en|RT mbchavez86 See...|                    |841809348628172800|\n|           null|Wed Mar 15 00:32:...|   1489537963433|  Silken_ebooks|  en|In reality though...|Pacific Time (US ...|841809348636639232|\n|           null|Wed Mar 15 00:32:...|   1489537963372|    kingmig360x|  en|jessiejensen_x ha...|                    |841809348380749826|\n|           null|Wed Mar 15 00:32:...|   1489537963367|   Tashinamuzik|  en|Frass face looks ...|Central Time (US ...|841809348359774211|\n|           null|Wed Mar 15 00:32:...|   1489537963542|     MomsGarage|  en|RT jugglers Get o...|                    |841809349093728256|\n|           null|Wed Mar 15 00:32:...|   1489537963606|   OhItsChespin|  en|This is the first...|                    |841809349362110465|\n|           null|Wed Mar 15 00:32:...|   1489537963563|  Adairbriones6|  en|RT cwatkins199 Ad...|                    |841809349181874176|\n|           null|Wed Mar 15 00:32:...|   1489537963539|      PaulSlack|  en|RT Labour4Animals...|                    |841809349081214976|\n|           null|Wed Mar 15 00:32:...|   1489537963626| kaystackzzzzzz|  en|RT ImtheMAINE Hap...|                    |841809349446144001|\n|           null|Wed Mar 15 00:32:...|   1489537963591|      succrenis|  en|RT gossiped when ...|Eastern Time (US ...|841809349299298304|\n+---------------+--------------------+----------------+---------------+----+--------------------+--------------------+------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1495680906438_592116279","id":"20170314-222922_910635069","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:13:26+0000","dateFinished":"2017-05-25T03:13:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:327"},{"text":"\nvar messages = tweetDF.select(\"msg\")\nprintln(\"Total messages: \" + messages.count())\n\nvar happyMessages = messages.filter(messages(\"msg\").contains(\"happy\"))\nval countHappy = happyMessages.count()\nprintln(\"Number of happy messages: \" +  countHappy)\n\nvar unhappyMessages = messages.filter(messages(\"msg\").contains(\" sad\"))\nval countUnhappy = unhappyMessages.count()\nprintln(\"Unhappy Messages: \" + countUnhappy)\n\nval smallest = Math.min(countHappy, countUnhappy).toInt\n\n//Create a dataset with equal parts happy and unhappy messages\nvar tweets = happyMessages.limit(smallest).unionAll(unhappyMessages.limit(smallest))\n    \n    ","user":"anonymous","dateUpdated":"2017-05-25T03:13:38+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nmessages: org.apache.spark.sql.DataFrame = [msg: string]\nTotal messages: 14440\n\nhappyMessages: org.apache.spark.sql.DataFrame = [msg: string]\n\ncountHappy: Long = 5351\nNumber of happy messages: 5351\n\nunhappyMessages: org.apache.spark.sql.DataFrame = [msg: string]\n\ncountUnhappy: Long = 1932\nUnhappy Messages: 1932\n\nsmallest: Int = 1932\n\ntweets: org.apache.spark.sql.DataFrame = [msg: string]\n"}]},"apps":[],"jobName":"paragraph_1495680906439_591731530","id":"20170314-222925_680225414","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:13:39+0000","dateFinished":"2017-05-25T03:13:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:328"},{"text":"val messagesRDD = tweets.rdd\n//We use scala's Try to filter out tweets that couldn't be parsed\nval goodBadRecords = messagesRDD.map(\n  row =>{\n    Try{\n      val msg = row(0).toString.toLowerCase()\n      var isHappy:Int = 0\n      if(msg.contains(\" sad\")){\n        isHappy = 0\n      }else if(msg.contains(\"happy\")){\n        isHappy = 1\n      }\n      var msgSanitized = msg.replaceAll(\"happy\", \"\")\n      msgSanitized = msgSanitized.replaceAll(\"sad\",\"\")\n      //Return a tuple\n      (isHappy, msgSanitized.split(\" \").toSeq)\n    }\n  }\n)\n\n//We use this syntax to filter out exceptions\nval exceptions = goodBadRecords.filter(_.isFailure)\nprintln(\"total records with exceptions: \" + exceptions.count())\nexceptions.take(10).foreach(x => println(x.failed))\nvar labeledTweets = goodBadRecords.filter((_.isSuccess)).map(_.get)\nprintln(\"total records with successes: \" + labeledTweets.count())\n","user":"anonymous","dateUpdated":"2017-05-25T03:13:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nmessagesRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[1261] at rdd at <console>:119\n\ngoodBadRecords: org.apache.spark.rdd.RDD[scala.util.Try[(Int, Seq[String])]] = MapPartitionsRDD[1262] at map at <console>:121\n\nexceptions: org.apache.spark.rdd.RDD[scala.util.Try[(Int, Seq[String])]] = MapPartitionsRDD[1263] at filter at <console>:123\ntotal records with exceptions: 0\n\nlabeledTweets: org.apache.spark.rdd.RDD[(Int, Seq[String])] = MapPartitionsRDD[1265] at map at <console>:123\ntotal records with successes: 3864\n"}]},"apps":[],"jobName":"paragraph_1495680906440_589807785","id":"20170314-234521_264925171","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:13:50+0000","dateFinished":"2017-05-25T03:13:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:329"},{"text":"labeledTweets.take(10).foreach(x => println(x))\n","user":"anonymous","dateUpdated":"2017-05-25T03:13:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(1,WrappedArray(rt, emmawood22, if, i, end, up, being, half, the, person, my, mom, is, ill, be, more, than))\n(1,WrappedArray(rt, aldubnationncr, prayer, is, powerful, its, amazing, what, it, can, do, always, stay, , at, all, timesrt, amp, spread, love, worldwide🌻🌻🌻…))\n(1,WrappedArray(i, dont, have, coworkers, so, i, ordered, fresh, baked, pastries, for, myself, because, im, a, greedy, glutton, im, also, …, https//tco/4f1afvfqhz))\n(1,WrappedArray(rt, hopeferrinson, be, a, , girl, like, me, 🌼, https//tco/xm5mmyrgwp))\n(1,WrappedArray(rt, stormzy1, do, what, makes, you, , mate))\n(1,WrappedArray(i, should, feel, , but, i, feel, like, something, bad, is, about, to, happenbut, what, will, happen, after, we, defeat, isis, https//tco/dswtkewmly))\n(1,WrappedArray(rt, ringelrei, please, i, want, them, to, be, , https//tco/mmms2l9zbw))\n(1,WrappedArray(rt, salvesayson, do, more, of, what, makes, you, , aldubxdtbytugofhearts))\n(1,WrappedArray(rt, kalebhorton, saying, america, is, optimistic, because, ceos, are, optimistic, is, like, saying, prisoners, are, , because, you, heard, the, w…))\n(1,WrappedArray(rt, netflixuk, make, your, own, , ending, ouat, https//tco/rpg2pzepub))\n"}]},"apps":[],"jobName":"paragraph_1495680906442_590577283","id":"20170315-221309_2084520780","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:13:59+0000","dateFinished":"2017-05-25T03:14:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:330"},{"text":"    val hashingTF = new HashingTF(2000)\n\n    //Map the input strings to a tuple of labeled point + input text\n    val input_labeled = labeledTweets.map(\n      t => (t._1, hashingTF.transform(t._2)))\n      .map(x => new LabeledPoint((x._1).toDouble, x._2))\n\n      ","user":"anonymous","dateUpdated":"2017-05-25T03:14:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nhashingTF: org.apache.spark.mllib.feature.HashingTF = org.apache.spark.mllib.feature.HashingTF@10a86291\n\ninput_labeled: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[1267] at map at <console>:129\n"}]},"apps":[],"jobName":"paragraph_1495680906444_588268790","id":"20170315-221527_265576053","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:14:03+0000","dateFinished":"2017-05-25T03:14:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:331"},{"text":"input_labeled.take(10).foreach(println)","user":"anonymous","dateUpdated":"2017-05-25T03:14:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(1.0,(2000,[105,299,361,367,571,801,817,931,1139,1357,1370,1477,1500,1525,1650,1739,1961],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[0,196,256,263,272,296,569,616,673,708,831,1123,1208,1211,1370,1371,1650,1757,1763,1858,1940],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[46,97,105,167,182,230,477,680,843,873,1191,1240,1297,1305,1364,1577,1676,1679,1824],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[0,97,353,1020,1139,1184,1480,1650,1751,1874],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[0,133,196,921,1211,1589,1650,1839],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[0,105,159,196,227,291,618,940,982,1285,1370,1410,1469,1490,1707,1751,1790,1921,1939,1940],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[0,105,367,496,940,1139,1650,1658,1707,1872],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[0,4,196,1211,1393,1525,1543,1589,1650,1839],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n(1.0,(2000,[0,182,381,801,852,1166,1243,1370,1650,1675,1735,1751,1839,1919,1926],[1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0]))\n(1.0,(2000,[0,56,361,470,757,1123,1650,1783,1854],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n"}]},"apps":[],"jobName":"paragraph_1495680906445_587884041","id":"20170315-225826_221402586","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:14:06+0000","dateFinished":"2017-05-25T03:14:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"text":"//We're keeping the raw text for inspection later\nvar sample = labeledTweets.take(1000).map(\n  t => (t._1, hashingTF.transform(t._2), t._2))\n  .map(x => (new LabeledPoint((x._1).toDouble, x._2), x._3))","user":"anonymous","dateUpdated":"2017-05-25T03:14:12+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sample: Array[(org.apache.spark.mllib.regression.LabeledPoint, Seq[String])] = Array(((1.0,(2000,[105,299,361,367,571,801,817,931,1139,1357,1370,1477,1500,1525,1650,1739,1961],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])),WrappedArray(rt, emmawood22, if, i, end, up, being, half, the, person, my, mom, is, ill, be, more, than)), ((1.0,(2000,[0,196,256,263,272,296,569,616,673,708,831,1123,1208,1211,1370,1371,1650,1757,1763,1858,1940],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])),WrappedArray(rt, aldubnationncr, prayer, is, powerful, its, amazing, what, it, can, do, always, stay, \"\", at, all, timesrt, amp, spread, love, worldwide🌻🌻🌻…)), ((1.0,(2000,[46,97,105,167,182,230,477,680,843,873,1191,1240,1297,1305,1364,1577,1676..."}]},"apps":[],"jobName":"paragraph_1495680906446_589038287","id":"20170315-225625_1635512137","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:14:12+0000","dateFinished":"2017-05-25T03:14:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:333"},{"text":"\n    // Split the data into training and validation sets (30% held out for validation testing)\n    val splits = input_labeled.randomSplit(Array(0.7, 0.3))\n    val (trainingData, validationData) = (splits(0), splits(1))","user":"anonymous","dateUpdated":"2017-05-25T03:14:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[1268] at randomSplit at <console>:131, MapPartitionsRDD[1269] at randomSplit at <console>:131)\n\n\ntrainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[1268] at randomSplit at <console>:131\nvalidationData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[1269] at randomSplit at <console>:131\n"}]},"apps":[],"jobName":"paragraph_1495680906448_599041759","id":"20170315-220757_1010718454","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:14:15+0000","dateFinished":"2017-05-25T03:14:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:334"},{"text":"\n    val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n    boostingStrategy.setNumIterations(20) //number of passes over our training data\n    boostingStrategy.treeStrategy.setNumClasses(2) //We have two output classes: happy and sad\n    boostingStrategy.treeStrategy.setMaxDepth(5) \n    //Depth of each tree. Higher numbers mean more parameters, which can cause overfitting.\n    //Lower numbers create a simpler model, which can be more accurate. \n    //In practice you have to tweak this number to find the best value.\n\n    val model = GradientBoostedTrees.train(trainingData, boostingStrategy)","user":"anonymous","dateUpdated":"2017-05-25T03:14:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nboostingStrategy: org.apache.spark.mllib.tree.configuration.BoostingStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@514bc0de,org.apache.spark.mllib.tree.loss.LogLoss$@54385a,100,0.1,0.001)\n\n\n\nmodel: org.apache.spark.mllib.tree.model.GradientBoostedTreesModel = \nTreeEnsembleModel classifier with 20 trees\n\n"}]},"apps":[],"jobName":"paragraph_1495680906455_597887512","id":"20170315-232951_1402100499","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:14:18+0000","dateFinished":"2017-05-25T03:17:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:335"},{"text":"// Evaluate model on test instances and compute test error\nvar labelAndPredsTrain = trainingData.map { point =>\n  val prediction = model.predict(point.features)\n  Tuple2(point.label, prediction)\n}\n\nvar labelAndPredsValid = validationData.map { point =>\n  val prediction = model.predict(point.features)\n  Tuple2(point.label, prediction)\n}\n\n//Since Spark has done the heavy lifting already, lets pull the results back to the driver machine.\n//Calling collect() will bring the results to a single machine (the driver) and will convert it to a Scala array.\n\n//Start with the Training Set\nval results = labelAndPredsTrain.collect()\n\nvar happyTotal = 0\nvar unhappyTotal = 0\nvar happyCorrect = 0\nvar unhappyCorrect = 0\nresults.foreach(\n  r => {\n    if (r._1 == 1) {\n      happyTotal += 1\n    } else if (r._1 == 0) {\n      unhappyTotal += 1\n    }\n    if (r._1 == 1 && r._2 ==1) {\n      happyCorrect += 1\n    } else if (r._1 == 0 && r._2 == 0) {\n      unhappyCorrect += 1\n    }\n  }\n)\nprintln(\"unhappy messages in Training Set: \" + unhappyTotal + \" happy messages: \" + happyTotal)\nprintln(\"happy % correct: \" + happyCorrect.toDouble/happyTotal)\nprintln(\"unhappy % correct: \" + unhappyCorrect.toDouble/unhappyTotal)\n\nval testErr = labelAndPredsTrain.filter(r => r._1 != r._2).count.toDouble / trainingData.count()\nprintln(\"Test Error Training Set: \" + testErr)\n\n\n\n//Compute error for validation Set\nval results = labelAndPredsValid.collect()\n\nvar happyTotal = 0\nvar unhappyTotal = 0\nvar happyCorrect = 0\nvar unhappyCorrect = 0\nresults.foreach(\n  r => {\n    if (r._1 == 1) {\n      happyTotal += 1\n    } else if (r._1 == 0) {\n      unhappyTotal += 1\n    }\n    if (r._1 == 1 && r._2 ==1) {\n      happyCorrect += 1\n    } else if (r._1 == 0 && r._2 == 0) {\n      unhappyCorrect += 1\n    }\n  }\n)\nprintln(\"unhappy messages in Validation Set: \" + unhappyTotal + \" happy messages: \" + happyTotal)\nprintln(\"happy % correct: \" + happyCorrect.toDouble/happyTotal)\nprintln(\"unhappy % correct: \" + unhappyCorrect.toDouble/unhappyTotal)\n\nval testErr = labelAndPredsValid.filter(r => r._1 != r._2).count.toDouble / validationData.count()\nprintln(\"Test Error Validation Set: \" + testErr)\n\n","user":"anonymous","dateUpdated":"2017-05-25T03:18:27+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlabelAndPredsTrain: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1809] at map at <console>:106\n\nlabelAndPredsValid: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1810] at map at <console>:107\nresults: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0...\nhappyTotal: Int = 0\n\nunhappyTotal: Int = 0\n\nhappyCorrect: Int = 0\n\nunhappyCorrect: Int = 0\nunhappy messages in Training Set: 1358 happy messages: 1358\nhappy % correct: 0.7172312223858616\nunhappy % correct: 0.927098674521355\n\ntestErr: Double = 0.22496318114874816\nTest Error Training Set: 0.22496318114874816\nresults: Array[(Double, Double)] = Array((1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (0.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (0.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0...\nhappyTotal: Int = 0\n\nunhappyTotal: Int = 0\n\nhappyCorrect: Int = 0\n\nunhappyCorrect: Int = 0\nunhappy messages in Validation Set: 603 happy messages: 545\nhappy % correct: 0.6807339449541284\nunhappy % correct: 0.8308457711442786\n\ntestErr: Double = 0.24041811846689895\nTest Error Validation Set: 0.24041811846689895\n"}]},"apps":[],"jobName":"paragraph_1495680906459_596348516","id":"20170315-233951_1248887406","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:18:27+0000","dateFinished":"2017-05-25T03:18:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:336"},{"text":"//Print some examples and how they scored\nval predictions = sample.map { point =>\n  val prediction = model.predict(point._1.features)\n  (point._1.label, prediction, point._2)\n}\n\n//The first entry is the true label. 1 is happy, 0 is unhappy. \n//The second entry is the prediction.\npredictions.take(100).foreach(x => println(\"label: \" + x._1 + \" prediction: \" + x._2 + \" text: \" + x._3.mkString(\" \")))","user":"anonymous","dateUpdated":"2017-05-25T03:19:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"predictions: Array[(Double, Double, Seq[String])] = Array((1.0,1.0,WrappedArray(rt, emmawood22, if, i, end, up, being, half, the, person, my, mom, is, ill, be, more, than)), (1.0,1.0,WrappedArray(rt, aldubnationncr, prayer, is, powerful, its, amazing, what, it, can, do, always, stay, \"\", at, all, timesrt, amp, spread, love, worldwide🌻🌻🌻…)), (1.0,0.0,WrappedArray(i, dont, have, coworkers, so, i, ordered, fresh, baked, pastries, for, myself, because, im, a, greedy, glutton, im, also, …, https//tco/4f1afvfqhz)), (1.0,1.0,WrappedArray(rt, hopeferrinson, be, a, \"\", girl, like, me, 🌼, https//tco/xm5mmyrgwp)), (1.0,1.0,WrappedArray(rt, stormzy1, do, what, makes, you, \"\", mate)), (1.0,0.0,WrappedArray(i, should, feel, \"\", but, i, feel, like, something, bad, is, about, to, happenbut, what, w...label: 1.0 prediction: 1.0 text: rt emmawood22 if i end up being half the person my mom is ill be more than\nlabel: 1.0 prediction: 1.0 text: rt aldubnationncr prayer is powerful its amazing what it can do always stay  at all timesrt amp spread love worldwide🌻🌻🌻…\nlabel: 1.0 prediction: 0.0 text: i dont have coworkers so i ordered fresh baked pastries for myself because im a greedy glutton im also … https//tco/4f1afvfqhz\nlabel: 1.0 prediction: 1.0 text: rt hopeferrinson be a  girl like me 🌼 https//tco/xm5mmyrgwp\nlabel: 1.0 prediction: 1.0 text: rt stormzy1 do what makes you  mate\nlabel: 1.0 prediction: 0.0 text: i should feel  but i feel like something bad is about to happenbut what will happen after we defeat isis https//tco/dswtkewmly\nlabel: 1.0 prediction: 1.0 text: rt ringelrei please i want them to be  https//tco/mmms2l9zbw\nlabel: 1.0 prediction: 1.0 text: rt salvesayson do more of what makes you  aldubxdtbytugofhearts\nlabel: 1.0 prediction: 1.0 text: rt kalebhorton saying america is optimistic because ceos are optimistic is like saying prisoners are  because you heard the w…\nlabel: 1.0 prediction: 1.0 text: rt netflixuk make your own  ending ouat https//tco/rpg2pzepub\nlabel: 1.0 prediction: 1.0 text: stephglitherow  birthday beautiful girl hope you have a lovely day and get abso spoilt lots of love 💗\nlabel: 1.0 prediction: 1.0 text: rt justagirithing someone when was the last time u were truly  me https//tco/ytsjnoreru\nlabel: 1.0 prediction: 1.0 text: rt hornyfacts i could chill at my boyfriends house amp he can spend 0 on me amp i would be  asf yall girls crazy\nlabel: 1.0 prediction: 1.0 text: rt brandonnembhard as long as youre  ill be fine\nlabel: 0.0 prediction: 0.0 text: rt glovisual i go from being  to being  in a split second and i dont know why\nlabel: 1.0 prediction: 1.0 text: rt daisymarquez_ forgot to post this on here but one last time  birthday to my baby i love you bunches💕 https//tco/hkuqwqp6yd\nlabel: 1.0 prediction: 1.0 text: i am  to announce that i will be continuing my academic and athletic career at dean college gobulldogs 🐾⚾\nlabel: 1.0 prediction: 1.0 text: rt shamakho_works 海未ちゃん描きました。 birthday🎉lovelive園田海未生誕祭2017 3月15日は園田海未の誕生日 https//tco/hajcpemlak\nlabel: 1.0 prediction: 1.0 text: mjffc essentials definitely always make me feel  listening to any of them mjs voice is so joyous 👍\nlabel: 1.0 prediction: 1.0 text: rt ltskermit so annoying when ppl are like ur always laughin u laugh at everything its not that funny like ok sorry im a  per…\nlabel: 1.0 prediction: 1.0 text: rt freddyamazin someone when was the last time u were truly me https//tco/0lg917uptk\nlabel: 1.0 prediction: 1.0 text: rt stormzy1 do what makes you  mate\nlabel: 1.0 prediction: 1.0 text: rt maywarddvo_thai trust yourself create the kind of self that you will be  to live with all your lifembmayward\nlabel: 1.0 prediction: 0.0 text: xiao is the best mommy and i have no no problem she has a lot to say to you and i hope youre so  to have you a queen queen of queen\nlabel: 1.0 prediction: 1.0 text: ashlyn_ruf  birthday\nlabel: 1.0 prediction: 1.0 text: im just a  fan girl cuz 2 chainz rt my tweet n jhene aiko favd it imfamous nah im jk but thats lit im lit now\nlabel: 1.0 prediction: 0.0 text:  tummy  soul 🤗🍴\nlabel: 1.0 prediction: 1.0 text: widget is going to lose despite earning the 3rd most votes in the entire field i know i should be  but its b… https//tco/rfggmibhpt\nlabel: 1.0 prediction: 0.0 text: rt peachmeadows only miserable people are bullys amp thats a fact no one who is  ever has the urge to bring others down remember t…\nlabel: 1.0 prediction: 0.0 text: which three countries do you want to visit next — im  where i am https//tco/dfq2hlxejc\nlabel: 1.0 prediction: 1.0 text: rt dillmaddie i just want to be\nlabel: 1.0 prediction: 1.0 text: rt karmenloveee if we date dont worry about me cheating im with you for a reason i only want you and when im not  ill let you…\nlabel: 1.0 prediction: 1.0 text: rt whitepplquote wishing their parents a  birthday on instagram or twitter when their parents dont have social media\nlabel: 1.0 prediction: 1.0 text: rt justagirithing someone when was the last time u were truly  me https//tco/ytsjnoreru\nlabel: 1.0 prediction: 0.0 text: ashleyrae_ thanks for sharing ashley  jamesonstpats\nlabel: 1.0 prediction: 0.0 text: broschan93 おはよう〜〜体調大丈夫ですか？季節の変わり目やら気圧の変化でこの時期は体調崩しがちだそうです、、気をつけて無理しないでくださいね。😊❣💖今日もで、行きましょう🎶💞🐶🐶 https//tco/8xhlbg7e0x\nlabel: 1.0 prediction: 1.0 text: the only thing im focused on is being genuinely  finding 2 new jobs and figuring out what college im going to all else is extra\nlabel: 1.0 prediction: 1.0 text: rt fiyingfuck i deserve to be  fuck all this bullshit\nlabel: 1.0 prediction: 1.0 text: harry_styles youre the reason of my smile i love you and im proud of you i would be very  if you follow me one day ❤️  751\nlabel: 1.0 prediction: 1.0 text: stephencurry30  birthday to you curry\nlabel: 1.0 prediction: 1.0 text: rt maywarddvo_thai trust yourself create the kind of self that you will be  to live with all your lifembmayward\nlabel: 1.0 prediction: 0.0 text: im finally  again😁\nlabel: 1.0 prediction: 1.0 text: ladyboss19023 newtgingrich not me im very  i enjoy life im not giving my time to haters\nlabel: 1.0 prediction: 1.0 text: rt stormzy1 do what makes you  mate\nlabel: 1.0 prediction: 1.0 text: ibisobacb  birthday tein much love hunnay ❤️❤️ https//tco/jtxsgv5kax\nlabel: 1.0 prediction: 1.0 text: rt ringelrei please i want them to be  https//tco/mmms2l9zbw\nlabel: 1.0 prediction: 1.0 text:  birthday danitriv miss amp love u sm 💕\nlabel: 1.0 prediction: 1.0 text: rt richthekid focus on what makes you\nlabel: 1.0 prediction: 0.0 text: rt manisexual they look so fucking  😭 https//tco/u82c32e8sc\nlabel: 1.0 prediction: 1.0 text: jungleboyj bwhugen i was going to say if you can make it through the storm were  to have you\nlabel: 1.0 prediction: 0.0 text: it is actually suuuuch a sick feeling going to bed feelin all loved and  😌😺👫🌟🍀👯🌸💘👨‍👩‍👧‍👦\nlabel: 1.0 prediction: 1.0 text: rt millennialofmnl this is your reminder that its okay to not be okay no one is constantly  your emotions are yours no one can t…\nlabel: 1.0 prediction: 1.0 text: rt soiute sometimes the best way to be  is to learn to let go of things you tried hard to hold on to that are no longer good for you\nlabel: 1.0 prediction: 0.0 text: rt salutemeimnia im so  that sayyora amp seven are now clicking the 2 prettiest in the house 💁🏼 bgc17\nlabel: 1.0 prediction: 0.0 text: forever  with who im with 💯\nlabel: 1.0 prediction: 1.0 text:  birthday joe hope your snow day was awesome 💙😻❄️💗 joe_liotti\nlabel: 1.0 prediction: 1.0 text: rt girlposts someone when was the last time u were truly me https//tco/wzhzedcf5p\nlabel: 1.0 prediction: 1.0 text: headbanshee_ small smirk at that im  to hear it\nlabel: 1.0 prediction: 0.0 text: 最近で一番だったかなー🎶\nlabel: 1.0 prediction: 1.0 text: sometimes life can surprise you with a  coincidence 😇✌ https//tco/hwojz90zjh\nlabel: 1.0 prediction: 1.0 text: zz_zt  to brighten your day 🙂\nlabel: 1.0 prediction: 1.0 text: rt shamakho_works 海未ちゃん描きました。 birthday🎉lovelive園田海未生誕祭2017 3月15日は園田海未の誕生日 https//tco/hajcpemlak\nlabel: 1.0 prediction: 0.0 text: rt purelydallas these are my favorite pictures ever look at how cute amp  they are https//tco/l64mhxw1r4\nlabel: 1.0 prediction: 0.0 text: rt gigglypjm the best concept  bangtan https//tco/otu3ceclvu\nlabel: 0.0 prediction: 0.0 text: rt fursonajail im just so  now all the fucking time because i know what its like to be  thankyoudan for making me h…\nlabel: 1.0 prediction: 1.0 text: so  to be offering this challenge again https//tco/xbhj6x0wxh https//tco/zl6qxucpr7\nlabel: 1.0 prediction: 1.0 text: mamakouk there should be more people as  as you share your avohappiness https//tco/8px885pate\nlabel: 1.0 prediction: 1.0 text: rt karhenzan jayreg1969 me too 😄 its an amazing photo xylobands makes you feel incredibly  and emotional https//tco/6o5mfimkic\nlabel: 1.0 prediction: 1.0 text: rt kestrelambrose “get yourself a black spiritual feminist from erykah badu twitter and be  like ace hood” 🤸🏾‍♀️ https//tco/t9rwn…\nlabel: 1.0 prediction: 1.0 text: brielle___  birthday and congrats 💜\nlabel: 1.0 prediction: 1.0 text:  birthday baby dirtywaternymph see u in july 💓\nlabel: 1.0 prediction: 1.0 text: rt stormzy1 do what makes you  mate\nlabel: 1.0 prediction: 1.0 text: tesko249 i found your long lost american brother now you can be a  family again https//tco/eluntmvgfz\nlabel: 1.0 prediction: 0.0 text: quiddicompare has just helped another  customer from didcot https//tco/7m78fpjq7f cleversearch getaloan\nlabel: 1.0 prediction: 1.0 text: went shopping today and for the first time in a long time i was not angry trying clothes on im so  with myself 16poundsdown💪🏽\nlabel: 1.0 prediction: 1.0 text: be  its one way of being wise sidonie gabrielle colette\nlabel: 1.0 prediction: 0.0 text: guys ive got 1/3 of an eggplant leftover if anyone wants it im more than  to ship it\nlabel: 1.0 prediction: 1.0 text:  birthday babe miss you so so much 😭💗 anjelika143\nlabel: 1.0 prediction: 1.0 text: rt iam_chenise benji_404  belated birthday jordie  i miss you lol\nlabel: 1.0 prediction: 1.0 text: amba_shaw  birthday beauty 🎉❤😘 xxxx\nlabel: 1.0 prediction: 1.0 text: maybe i brag about how  we are too much so to even it out ill let you in on a secret it is constant farts around here just constant\nlabel: 1.0 prediction: 1.0 text: well i just saw my rims got picked up today by ups so km  theyre on the way to me so im  if anyone cares 😊😊😊\nlabel: 1.0 prediction: 1.0 text: rt elliewineman  birthday cutie hope ur day was unreal 💘💘jordezzle\nlabel: 1.0 prediction: 0.0 text: were  people hate that ☺️\nlabel: 1.0 prediction: 1.0 text:  birthday to my brother for life you know how much love i got for you spring break is up tagumss https//tco/jmtjo2etmc\nlabel: 1.0 prediction: 1.0 text: rt unbibs its 3/14  pi π 314159265359 day https//tco/qsmtqi9mxj\nlabel: 1.0 prediction: 1.0 text: rt thepoemstext do what makes you  be with who makes you smile laugh as much as you breathe and love as long as you live\nlabel: 1.0 prediction: 0.0 text: rt lingerie_addict in an industry thats all to  to discard women once they reach the age of 25 campaigns like this are necessary…\nlabel: 1.0 prediction: 1.0 text: rt rm_archive they make me so  😊 i love best friends https//tco/dqszn1arr8\nlabel: 1.0 prediction: 1.0 text: joannajohnson31 stef and lena deserves to be   thefosterschat thank you for this amazing show❤ https//tco/4ua1m9qlyh\nlabel: 1.0 prediction: 1.0 text: rt julietbienias zacharymanuel  birthday\nlabel: 1.0 prediction: 1.0 text: rt justagirithing someone when was the last time u were truly  me https//tco/ytsjnoreru\nlabel: 1.0 prediction: 1.0 text: rt marksonchu 151 a hurt yugyeomie makes a  jinyoungiehttps//tco/u5s8l10w8w\nlabel: 1.0 prediction: 1.0 text: rt ohteenquotes spend life with who makes you  not with who you have to impress\nlabel: 1.0 prediction: 1.0 text: rt andrea_mxreno amandaxxmar1e  birthday beautiful❤❤\nlabel: 1.0 prediction: 1.0 text: rt miakhallfa dating me is simple just text me back fast make me  amp dont lie to me amp there wont be any problems\nlabel: 1.0 prediction: 1.0 text: rt stormzy1 do what makes you  mate\nlabel: 1.0 prediction: 1.0 text: rt danameeli they not bro 😂 niggas cant handle that spanish life get ya self one and youll be  https//tco/5zinxdvpsm\nlabel: 1.0 prediction: 1.0 text: rt de4d_ringer  pi day im still depressed\nlabel: 1.0 prediction: 1.0 text: rt junethejenius be  with what you have while working for what you want\n"}]},"apps":[],"jobName":"paragraph_1495680906462_595194270","id":"20170315-235355_1294226754","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:19:19+0000","dateFinished":"2017-05-25T03:19:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:337"},{"text":"model.save(sc, \"hdfs:///tmp/tweets/RandomForestModel\")","user":"anonymous","dateUpdated":"2017-05-25T03:08:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://sandbox.hortonworks.com:8020/tmp/tweets/RandomForestModel/metadata already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1177)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1464)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1443)\n\tat org.apache.spark.mllib.tree.model.TreeEnsembleModel$SaveLoadV1_0$.save(treeEnsembleModels.scala:447)\n\tat org.apache.spark.mllib.tree.model.GradientBoostedTreesModel.save(treeEnsembleModels.scala:131)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:89)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:91)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:93)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:95)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:97)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:99)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:101)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:103)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:105)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:107)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:111)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:113)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:115)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:117)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:119)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:121)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:123)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:125)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:127)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:129)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:131)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:133)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:135)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:137)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:139)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:141)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:143)\n\tat $iwC$$iwC$$iwC.<init>(<console>:145)\n\tat $iwC$$iwC.<init>(<console>:147)\n\tat $iwC.<init>(<console>:149)\n\tat <init>(<console>:151)\n\tat .<init>(<console>:155)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:972)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:1198)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:1144)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:1137)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:489)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolEx\n\n\n\necutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"}]},"apps":[],"jobName":"paragraph_1495680906471_579419565","id":"20170315-235824_1541028619","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:12:57+0000","dateFinished":"2017-05-25T03:12:58+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:338"},{"text":"\nprintln(model.predict(hashingTF.transform(\"Happy \".split(\" \").toSeq)))","user":"anonymous","dateUpdated":"2017-05-25T03:19:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"0.0\n"}]},"apps":[],"jobName":"paragraph_1495680906473_577111071","id":"20170316-000532_1106625181","dateCreated":"2017-05-25T02:55:06+0000","dateStarted":"2017-05-25T03:19:34+0000","dateFinished":"2017-05-25T03:19:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:339"},{"user":"anonymous","dateUpdated":"2017-05-25T03:08:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1495680906475_577880569","id":"20170320-195111_1459375677","dateCreated":"2017-05-25T02:55:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:340"}],"name":"Sentiment Analysis Spark","id":"2CG864CH6","angularObjects":{"2CEW5GBT8:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CGSU7XSZ:shared_process":[],"2CJCXYWM2:shared_process":[],"2CGGK94TT:shared_process":[],"2CFK35D2R:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CFNPFVQG:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}